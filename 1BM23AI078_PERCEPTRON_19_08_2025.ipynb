{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax_all(outputs):\n",
    "    e_x = np.exp(outputs - np.max(outputs))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, activation='identity', learning_rate=0.1, epochs=100):\n",
    "        self.weights = np.zeros(input_size + 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.activation_name = activation\n",
    "\n",
    "        if activation == 'linear':\n",
    "            self.activation = linear\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = None  \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1) \n",
    "        z = np.dot(self.weights, x)\n",
    "        if self.activation_name != 'softmax':\n",
    "            return self.activation(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                xi_aug = np.insert(xi, 0, 1) \n",
    "                z = np.dot(self.weights, xi_aug)\n",
    "                output = self.activation(z) if self.activation_name != 'softmax' else z\n",
    "                error = target - output\n",
    "                self.weights += self.learning_rate * error * xi_aug\n",
    "\n",
    "\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y = np.array([0, 0, 0, 1]) \n",
    "\n",
    "input_labels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "x_pos = range(len(input_labels))\n",
    "\n",
    "\n",
    "activations = ['linear', 'sigmoid', 'tanh', 'relu', 'softmax']\n",
    "\n",
    "# Prepare plots\n",
    "fig, axs = plt.subplots(1, len(activations), figsize=(5 * len(activations), 5))\n",
    "\n",
    "for i, activation_function in enumerate(activations):\n",
    "    perceptron = Perceptron(input_size=2, activation=activation_function, learning_rate=0.1, epochs=1000)\n",
    "    perceptron.train(X, y)\n",
    "\n",
    "    raw_outputs = [perceptron.predict(xi) for xi in X]\n",
    "\n",
    "    if activation_function == 'softmax':\n",
    "        outputs = softmax_all(np.array(raw_outputs))\n",
    "    else:\n",
    "        outputs = raw_outputs\n",
    "\n",
    "    print(f\"\\n=== Activation Function: {activation_function.upper()} ===\")\n",
    "    for xi, output in zip(X, outputs):\n",
    "        print(f\"Input: {xi}, Output: {output:.4f}\")\n",
    "\n",
    "    axs[i].plot(x_pos, outputs, marker='o', linestyle='-', color='tab:blue')\n",
    "    axs[i].set_title(f\"{activation_function.upper()} Activation\")\n",
    "    axs[i].set_xticks(x_pos)\n",
    "    axs[i].set_xticklabels(input_labels)\n",
    "    axs[i].set_ylim([-0.2, 1.2] if activation_function != 'tanh' else [-1.2, 1.2])\n",
    "    axs[i].set_xlabel(\"Inputs\")\n",
    "    axs[i].set_ylabel(\"Output\")\n",
    "    axs[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
